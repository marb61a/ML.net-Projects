Notes from Rubikscode ML.net course
    - https://rubikscode.net/courses/ml-net-full-stack-machine-learning-for-net-developers/
    - Machine Learning for .Net developers
    - https://docs.microsoft.com/en-us/dotnet/machine-learning/
    - https://github.com/dotnet/machinelearning
    - I have started using NB prefix for projects when using interactive notebooks

Complete suite of .NET versions of ML\AI libraries
    - https://scisharp.github.io/SciSharp/
    - https://github.com/SciSharp
    - https://github.com/dotnet/TorchSharp

Nimbus ML provides python bindings for ML.NET
    - https://docs.microsoft.com/en-us/nimbusml/overview
    - https://github.com/microsoft/NimbusML

Using Model Builder in VSCode
    - https://www.youtube.com/watch?v=R8aCkjSTSuc

I am very interested in ML.net as .net is used in RPA tools which gives some benefits
    - Learning C# based machine learning which can be used not only in RPA tools
    - https://docs.uipath.com/ai-fabric/v0/docs/building-ml-packages

There are some pre-requisites for the course
    - VSCode and Visual Studio 2022
        - .net 6 comes with the IDE installation
    - .net Interactive VSCode extension
    - The notebooks involved are similar to Jupyter notebooks
    - C# will be used instead of Python but will be used in an almost script-like manner

There are some datasets that will be used within the course
    - Palmer Penguins Dataset, Boston Housing Dataset, Netflix Prize Dataset, IMDb Sentiment Dataset and Cats vs Dogs

History of Machine Learning
    - https://www.techtarget.com/whatis/A-Timeline-of-Machine-Learning-History
    - https://www.dataversity.net/a-brief-history-of-machine-learning/
    - Machine Learning is no longer an obscure topic
    - Data volumes have increased by orders of magnitude in the last few years
        - By 2025 expected to be around 200 Zettabytes
        - A zettabyte is 1,000,000,000,000,000,000,000 or 10 to the power of 21
    - Machine Learning and Deep Learning are very in demand skills
    - A quick definition of Machine Learning
        - Machine Learning is the branch of computer science which uses statistical techniques to give computers the ability to learn
    - Learning in this context means gaining the ability to perform tasks to success rates through experience
    - Machine Learning tries to solves problems that are tough to solve by people or standard software
        - Problems include classification, regression, forecasting and anomaly detection

Gradient Descent
    - https://en.wikipedia.org/wiki/Gradient_descent
    - https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
    - Gradient Descent is the essential optimization technique and one of the main ingredients of some algorithms
        - It is the most popular optimisation technique and the basis for all others

Performance Metrics
    - Evaluation metrics are in general grouped by the task being solved by an algorithm
        - For example binary classification uses a different set of metrics to determine algorithm performance than would be used in regression
    - https://neptune.ai/blog/evaluation-metrics-binary-classification
    - There are some terms to consider when assessing classification model performance
        - Confusion Matrix or Error Matrix
            - https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/
            - Used to describe classification model performance when true values are known
        - Precision is another technique
            - It also carries more information than accuracy
            - It asks the question about what proportion of positive identications were correct
            - The value goes from 0 to 1 and the closer to 1 the better
        - Recall which is described as the ability of the classifier to find all the positive samples
            - This metric tries to answer the question, What proportion of actual positives was identified correctly
            - It is very similar to precision but differs slightly as precision measure result relevance
            - Recall on the other hand measures how many truly relevant results are returned
        - Accuracy is calculated as a number of correct predictions divided by the total number of predictions
            - This can be trick as it may lead to incorrect impressions about the model especiall when using imbalanced data
            - This means thet if the model performs well on the dominant class in the dataset it may not perform as well on others
            - Models that overfit have 100% accuracy
        - F1 Score is probably the most popular metric that combines precision and recall
            - It represents the harmonic mean of both
            - Its formula is (Precision * Recall / Precision + Recall) * 2 
        - Receiver Operating Characteristic (ROC) curve & Area Under the curve (AUC)
            - When an ML algorithm is predicting the class of a sample it calculates the probability that it belongs to a certain class
            - If the value is above a certain threshold then it is label as that class
            - ROC curves show the true positive rate against the false positive rate for certain thresholds 
            - The AUC metric is used as a measure of performance
            - This means that ROC is a probability curve where AUC measures the separability
            - This combination tells the model to  distinguish classes and the higher the value the better
        - Area under Precision-Recall Curve (AUPRC)
            - In order to correctly evaluate a model, both metrics need to be taken into consideration
            - Unfortunately as precision improves it usually reduces recall and the opposite is also true
            - The precision-recall curve shows the tradeoff between precision and recall
            - Areas under the curve represents both high recall and high precision
            - High scores for both show that the classifier is returning accurate results with a majority of positive results
    - Multi-Class Classification Metrics
        - Using multi-class classification we can use some of the same metrics that we use for binary classification
        - There are some differences for multi-class classification though
        - Confusion Matric, the difference when being used with multi-class is that each class has a row
        - Micro and Macro Accuracy
            - An accuracy formula can be applied for binary classification for eash class in a dataset
            - There are 2 different ways, the first is when all classes are treated equally, the metric is computed independently for each class and the average is taken
            - This first method is called Macro accuracy, the second is called Micro Accuracy
            - This is when we aggregate the contributions of all classes to compute the average metric
            - Micro Accuracy is a more reliable metric as it tries to account for any class imbalance
        - Log-Loss & Log-Loss Reduction
            - This is one of the most commonly used metrics in kaggle competitions
            - Log Loss is a metric that quantifies the accuracy of a classifier by penalizing false classifications
            - This metric’s value represents the amount of uncertainty of our prediction based on how much it varies from the actual label
            - Log-Loss Reduction is also called reduction in information gain – RIG
            - It gives a measure of how much a model improves on a model that gives random predictions
            - The close that RIG is to 1 the better the model
    - Regression Metrics 
        - Goals from regression problems differ to classification problems so different metrics are needed
        - Regression output is always continuous and the metrics need to match this
        - Mean Absolute Error – MAE
            - This calculates the average absolute distance (error) between predicted and targeted values
            - The closer to 0 the better, this also deals with outliers fairly well
        - Mean Squared Error – MSE
            - This is probably the most popular metric of all regression metrics
            - This calculates the average squared distance (error) between predicted and targeted values
            - Results are non-negative values and the goal is to get this value as close to zero as possible
            - This function is often used as a loss function of a machine learning model
        - Root Mean Squared Error – RMSE
            - This is a variation of the MSE metric
            - It shows what is the average deviation in predictions from the actual values
            - It makes and assumption that errors are unbiased and follows normal distribution 
            - RMSE is also a non-negative value and ) is the ideal value to be achieved
            - Lower RMSE values are better than higher
            - Outliers do affect this metric so a dataset must have them removed prior to using this metric
        - R Squared
            - R Squared is used when a more intuitive approach than MSE or RMSE is needed
            - It is also known as the coefficient of determination and is very popular
            - https://en.wikipedia.org/wiki/Coefficient_of_determination

How does ML.NET fit
    - ML.NET is Microsoft's ML framework which allows for model running, training etc within the .net ecosystem
    - ML.NET is faster that Pytorch using CPU
    - ML.NET was designed to be intuitive for .NET developers
    - The DataView class borrows from database concepts and is the reason why ML.NET is fast
    - ML.NEt performance on the GPU is not as good as Pytorch
    - Data Preprocession can be complicated
    - There not that many tools for data visualisation


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                LINEAR REGRESSION
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - https://machinelearningmastery.com/linear-regression-for-machine-learning/
    - The output of solving a regression problem is a continuous numerical value
        - One of the most famous is predicting the price of a house based on tenants ages which are independent variables
    - There are multiple sources for learning about the mathematics involved in linear regression
        - https://www.analyticsvidhya.com/blog/2021/08/understanding-linear-regression-with-mathematical-insights/
    - Multiple linear regression uses the reknowned Boston housing dataset
        - https://corporatefinanceinstitute.com/resources/knowledge/other/multiple-linear-regression/

Using Notebooks for Linear Regression
    - There are several steps needed
    - Install packages (NNuGet) and add using directives
    - Load and split the data
    - Train the model
    - Evaluate the model
    - Save the model
    - Load and predict

ML.NET Linear Regression
    - Online Gradient Descent
        - This is a variation of Stochastic Gradient Descent
        - It has a choice of loss functions and an option to update the weight vector which uses the vector average seen over time
    - SDCA which stands for Stochastic Dual Coordinate Ascent
        - It is another variation on Stochastic  Gradient Descent
        - This is suitable for large datasets as it can be scaled easily

Using C# Notebooks in VSCode
    - Files will still have the .ipynb extension
    - The kernel that will be used though is .NET interactive rather than a Python kernel
    - When installing packages use #r and then package name
        - In python notebooks this is done with an !
    - When loading a csv file
        - Use LoadFromTextFile<> which is part of MLContext.Data
        - This takes a couple of parameters, the name of the class (type) between the <>
        - The file path and whether the file has headers in the following brackets
            - Using the hasHeader directive which is set to true or false
        - Then the file delimiter is added using the separatorChar directive followed in this case by a comma
    - Splitting a dataset into testing and training
        - Again use MLContext.Data this time adding the TrainTestSplit method
        - The first parameter is the dataview created when loading the data
        - The second parameter is the amount of data to be used for testing using the testFraction directive which is set to 0.2 which equals 20%
        - The training dataset can be examined by using the variable that is assigned to the split and adding the TrainSet method for the training data
        - Then chain the ToTabularDataResource method on
        !!!! - Using a semi-colon will prevent the table from displaying
        - The test portion of the dataset can be loaded similar to above but using the TestSet method
    - Training the model
        - This uses the MLContext followed by Regression which then has trainers and finally the algorithm which is being used
        - In this case it is the SDCA algorithm being used to train the data
        - This takes 2 parameters which are label column name and feature
            - A feature column is added using the featureColumnName which is then given a value
            - A label column is defined using the labelColumnName directive which is then given a value
        - The dataset will need to be normalised prior to training
            - Training is a process which puts everything into the same scale
            - Differing scales can cause issues when training
            - The rivercoast feature is categorical which is a discrete\categorical value
        - We can create a piepline where all of the needed normalisation can be done
            - These can be done with MLContext Transforms
            - To transform categorical data use OneHotEncoding
                - One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms
                - https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f
            - After categorical data has been encoded then each of the features can be added using the Concatenate method available to Transforms
            - Then it is time for normalisation this time using NormalizeLpNorm
                - This takes 2 parameters which are source and destination, in this case "Features" for both
        - Once the pipeline has been set up then it can be used to train the model
            - The fit method is called then to train the training portion of the dataset
    - Evaluate the model
        - After the model has been trained then it is time for its performance to be evaluated
        - Using the trainedModel and Transform which takes the dataset TestSet
        - Evaluation can then be done with MLContext Regression and the Evaluate method which takes the variable that the transform operations have been assigned to
        - These metrics are MSE, MAE, RMSE, R Squared and Loss function
    - Save the model
        - Once the model has been trained it is time to save it for further use into the future
        - This is done using MLContext.Model.Save and takes some parameters strating with the trainedModel
        - The next parameter is the data schema which is needed, in this case the training data schema
        - The final parameter is the location the model will be saved, the model will always need a .mdl file extension
    - Load the model and Predict
        - This loads the model from the created file and can be used for predictions
        - The first step is to create a new sample which can be used to see if the model will predict correctly
        - In this case the first sample from the boston dataset serves well
        - Then the model is loaded using filestream and a using statement
        - The various file methods can then be used, FileMode set to open, FileAccess set to read and the same for FileAccess
        - This allows for opening the file, reading it and then again subsequently
        - Then load the model using MLContext.Model.Load
        - After the model has been loaded then the prediction engine needs to be created once again using methods from the model catalogue
        - Once the prediction engine has been put in place it can then be used with the prediction method and the new sample for a prediction


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                CLASSIFICATION
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623
    - The section will cover both binary and multiclass classification using both notebooks and normal projects
    - Classification algorithms build decision boundaries implicitly or explicitly
        - These boundaries can be straight, curved or have complex forms
        - The form of the boundary will determine the accuracy of the model
    - Classification works with discrete values, these are not continuous like regression
        - A person either is a gender or not, speaks a language or not etc
Binary classification notebook
    - This uses the penguins size dataset
    - The same packages as regression will need to be installed
    - Again the .NET interactive kernel is used
        - If not set then python will try to run those cells which cause errors
    - The same using directives are also needed at this point
    - Not all features will be used in the binary classification notebook example
    - After creating the class for holding the features that are needed another class is added for predictions
        - This will have a boolean method as classification is inherently true or false
    - Then it is time to import the data
        - Again as previously MLContext is used, this is used with the LoadFromTextFile method
        - This is a template function so the type wanted needs to be added which is the first class created
        - The parameters are then added starting with the data location and then whether there are headers and the separating character
    - After the data has be imported then it is time to split the data into test and train portions
        - In this example this will be 0.3 or 30%
        - This can be viewed using the ToTabularDataResource method again remebering to have no semi-colon to ensure rendering of table
    - The algorithm that will be used in modelling in this section is LBFGS
        - It is a variation on the the logistic regression that is based on the limited memory Broyden-Fletcher-Goldfarb-Shanno hence L-BFGS
        - https://en.wikipedia.org/wiki/Limited-memory_BFGS
    - The model will use MLContext.BinaryClassification.Trainers.LbfgsLogisticRegression
        - The name of the features column and label column will need to be set here as it was in the regression example
    - After stting up the model then it is time to construct a pipeline
        - This follows much tthe same process as seen previously
        - Again the data needs to be normaised this time using the NormalizeMinMax function
        - This is based on using the minimum and maximum values as a guide
        - There is also a need for source and destination parameters (Both can be set to "features")
        - Finally append the model to the pipeline
    - After setting up the pipeline then it is time to train the model
        - This is done using the Fit method
    - Then it is time to evaluate, first the test portion of the dataset needs to be transformed
        - Then this transformed data is passed to an Evaluation function which is part of the MLContext.BinaryClassification catalogue
        - The accuracy can be seen by printing out the metrics result and using its Accuracy method
    - After evaulation the result is about 96%
        - This means that it is good enought to be saved to disk
        - This saving and reloading will be the same as for the regression project

Standard C# Binary Classification
    - The projects that are using standard C# development techniques will not have a prefix, notebook projects begin with NB
        - There can be a standard console application used for this project
    - Firstly install a console application into the folder using the following command 
        - dotnet new console --framework net6.0
    - Once that is done then add in the data which will be in a data folder, again the PalmerPenguins dataset will be used
    - Then it is time to start adding in files and folders first up is a usings file to list all the using statements in the project
        - Each statement will be accompanied by a global keyword to make the usings available to all files
        - https://www.c-sharpcorner.com/article/global-using-directive-in-c-sharp-102/
    - Then a MachineLearning folder will be added, it will have several subfolders
        - DataModels which have 2 files, one is where we setup the type using LoadColumns to setup the appropriate structure
        - The second file will be the file that is used when predicting, it only needs a single variable
    - Then there is Common folder which contains the ITrainerBase class 
        - C# interfaces contain declarations of methods, properties, indexers, and events
        - https://www.tutorialsteacher.com/csharp/csharp-interface
    - After the interface comes the implementation class, this time called TrainerBase
        - This class is public and will need to use the items from the interface
        - https://www.w3schools.com/cs/cs_interface.php
        - It will need to implement all items from the interface to avoid errors
    -  This project makes use of several trainers including the LbfgsLogisticRegression
        - Others are AveragedPerceptronTrainer
        - PriorTrainer
        - SDCALogisticRegressionTrainer
        - SDCANonCalibratedTrainer
        - SGDCalibratedTrainer
        - SGDNonCalibratedTrainer

Multiclass Classification notebook
    - There are different algorithms support when running multiclass classification
        - LBFGS Maximum Entropy
            - This maximum entropy model differs from logistic regression in the number of classes supported
            - Logistic regression is used for binary classification whereas the maximum entropy handles multiple classes
            - It is also based on the Broyden-Fletcher-Goldfarb-Shanno method
        - Naive Bayes
        - One versus all
            - This performs binary classification for each class of the dataset and then creates multiple binary classifiers
            - Predictions are then made by running binary classifiers against each other to find the one with the highest confidence score
        - SDCA Maximum Entropy
            - This is also based on logistic regression
        - SDCA Non-Calibrated which is the uncalibrated version of the above algorithm
    - This uses a slightly different version of the penguins dataset
        - There are no 0 and 1 values for species
        - This is the full dataset rather than the subset previously used
    - This dataset does not have the species column which means the there will  need to be a conversion of the first column
        - This column is the species label but needs to be in numerical form when building the pipeline
        - This is handled by using mlContext.Transforms.Conversion.MapValueToKey() where MapValueToKey maps the text value to a numerical value
        - This takes 2 parameters an input column name and an output column name

Multiclass Classification Project (Non-notebook)
    - This is also very similar to the binary classification project
    - Again use the following command to create a new console project
        - dotnet new console --framework net6.0  
        - Manifests and packages will have to be installed
            - dotnet new tool-manifest
            - dotnet add package Microsoft.ML 
            - dotnet add package MathNet.Numerics
    - Subtle differences only such as the Evalute methods
        - In the binary classification used mlContext.BinaryClassification.EvaluateNonCalibrated
        - In the multiclass classification it was mlContext.MulticlassClassification.Evaluate
        - very large parts of the project was the same except changing names to multiclass instead of binary


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                DECISION TREES
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - https://en.wikipedia.org/wiki/Decision_tree_learning
    - https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/
    - There are multiple algorithms available but CART is covered here
    - Impurity is defined as a chance of being incorrect if you assign a label to an example at random  
        - A node is pure if all training instances it applies to belong to the same class
        - You therefore cannot makke a mistake when assigning a label to a random sample
        - https://www.learndatasci.com/glossary/gini-impurity/

Decision Tree Classification Notebooks
    - There are several decision tree classification algorithms supported by ML.NET
        - Fast Tree which is an implementation of the MART algorithm which stands for Multiple Additive Regression Trees
            - This algorithm has been known to deliver high prediction accuracy and is widely used
            - MART is an ensemble model of boosted regression trees, which means using gradient boost for calculations
            - It builds each regression tree in a stepwise fashion using a loss function to measure the error for each step which is corrected in the next
            - In regression boosing builds a series of trees step wise and selects the optimal tree
            - https://analyticsindiamag.com/a-beginners-guide-to-bayesian-additive-regression-trees/
        - Fast Tree Tweedie
            - This is at it's heart similar to Fast Tree but it uses a different gradient boosting algorithm
        - GAM or Generative Additive Models are usually implemented with decision trees 
            - It treats data as a set of linearly independent features
            - https://en.wikipedia.org/wiki/Generalized_additive_model
    - Needs the Microsoft.ML.FastTree library installed
        -A lot of the notebook from the binary classification notbook can be reused
    - When training the model the FastTree trainer is used and takes several parameters
        - The first parameter is the number of leaves in this case it is set to 3
        - The second parameter is the number of trees in this case it is also set to 3
        - The third parameter is the learning rate which determines how fast a model learns
            - https://en.wikipedia.org/wiki/Learning_rate

Decision Tree Classification
    - Again this follow the previous projects ways of doing things
    - Start off by creating a new directory and creating a console project inside it
    - There needs to be a separate install ation of the FastTree package
        - dotnet add package Microsoft.ML.FastTree
        - https://rubikscode.net/2021/04/26/machine-learning-with-ml-net-sentiment-analysis/
        - https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.trainers.fasttree?view=ml-dotnet

Decision Tree Regression Notebook
    - This will use the fasttree algorithm in the context of a regression problem
    - Installation packages are the same as for classification
    - Dataset used is the boston housing dataset
    - This will be very similar to the LinearRegression project
    - When normalizing the data the NormalizeLogMeanVariance normaliser is used
        - https://towardsdatascience.com/how-to-differentiate-between-scaling-normalization-and-log-transformations-69873d365a94
    - Increasing the number of trees when training the model will impace on the final prediction
        - 18.5 with 3 trees compared to 27.98 for 10 trees 

Decision Tree Regression
    - This will use the fasttree algorithm in the context of a regression problem
    - The FastTree package will need to be installed separately in the project
    - The boston house prices dataset will be used
    - One of the trainer files uses the FastTree Tweedie algorithm
    - Again had to rename the training file by adding a u to indicate user made
        - This is to prevent any errors due to ambiguity where file names are the same as library file names


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    RANDOM FOREST
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - Ensemble Learning
        - Sometimes in ML better results can be gotten by using multiple predictors and averaging results rather than one algorithm
        - Using multiple algorithms is referred to as ensemble learning
        - https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/
        - https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f
        - Ensemble Learning is based on the law of large numbers
            - https://en.wikipedia.org/wiki/Law_of_large_numbers
    - Hard Voting
        - Hard Voting is a type of ensemble learning
        - https://en.wikipedia.org/wiki/Law_of_large_numbers
        - If we take several classifiers voting for a class, the class that gets the majority of votes is the output
        - An ensemble containing 1000 classifiers, each has 51% accuracy alone, using ensemble hard voting can have accuracy of 75%
    - Soft Voting
        - Each of the algorithms will output a probability
        - The ensemble will predict the class with the highest class probability averaged over each of the classifiers
    - Bagging and Pasting
        - https://medium.com/@silvaan/ensemble-methods-bagging-and-pasting-in-scikit-learn-723f4183cdf4
        - This is where the same algorithm is used on different subsets of the training dataset
        - It is one of the most popular ways to build ensembles
        - There is only one major difference between bagging and pasting
            - Bagging allows training instances to be sampled several times for the same predictor where pasting does not
        - When all algorithms are trained the ensemble makes a prediction by aggregating the predictions of all algorithms
    - Feature Bagging
        - This is where during training each tree is trained on a subset of features
        - This in turn will lead to lower variance of the complete model

Random Forest Classification Notebook
    - This will cover Random Forest algorithms for classification
    - ML.NET supports Random Forest for both regression and classification
    - Random Forest classification is limited to binary classification
    - The Random Forest algorithm is called Fast Forest and is an ensemble build of Fast Tree
    - This n will reuse the penguins binary dataset
    - When training the model using FastForest there is no Learning_rate specified

Random Forest Classification Project
    - Again this will be a command line project using the DOTNET cli
    - The dataset being used will be the penguis binary dataset
    - The only trainer will be a Random Forest trainer file

Random Forest Regression Notebook
    - This time Random Forest will be assed for regression purposes
    - The Boston housing dataset will be used
    - When training the number of leaves is set to 30 and number of trees to 100
    - Once again with FastForest a learning rate is not needed

Random Forest Regression Project
    - Using the Boston housing dataset
    - Similar to both the classification project and other regression projects


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    SUPPORT VECTOR MACHINE
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47
    - SVM is one of the most popular ML algorithms
    - SVM observes every feature vecor as a point in a high dimensional space
    - It puts all feature vectors on an imaginary n-dimensional plot
    - It then draws an imaginary n-dimensionl line called a hyperplane
    - This separates examples with positive labels from negative labelled examples when dealing with classification
    - When dealing with regression it collects as much sampling as possible
        - https://towardsdatascience.com/unlocking-the-true-power-of-support-vector-regression-847fd123a4a0
    - SVM does not only create a hyperplane
        - There are also additional vectors constructed which define the margin
        - These vectors are called support vectors and the difference between the 2 closest classes is called the margin
        - Hyperplanes with support vectors is often called street
        - https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496
    - Kernel trick
        - This technique gives you the possibility to get the same result as if polynomial features were being used without using them
        - Kernels are just functions that map low-dimensional non-linearly seperable data into the opposite (high-dimensional linearly-separable data)
        - One example is mapping 2d non-linearly separable data into 3d separable data
        - https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f

Support Vector Machine Notebook
    - Linear SVM
        - This is an ML.NET implementation of the PEGASOS algorithm
        - https://qiskit.org/documentation/machine-learning/tutorials/07_pegasos_qsvc.html
    - Local Deep SVM   
        - This implementation is a generalisation of Localised Multiple Kernel Learning for non-linear SVM
        - LD-SVM speeds sup non-linear SVM which tend to be slower
        - Non-linear SVM uses kernel learning

Support Vector Machine Project
    - Again this will be a cli project
    - It will be a classification project
    - The architecture will be the same as other project folders
    - The trainers in this project will be both LinearSVM and LdSVM


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    CLUSTERING
--------------------------------------------------------------------------------------------------------------------------------------------------
This lesson is where the shift away from supervised learning begins
    - There will be 3 clustering algorithms covered,  K-Means Clustering, Agglomerative Clustering and DBSCAN
Intuition
    - Unsupervised Learning
        - In real life there is only input data available
        - This means that the algorithms that we use need to figure out any connections between input samples
        - This is where unsupervised learning comes in
        - Supervised learning can solve regression and classification problems but unsupervised learning solves clustering problems
    - Clustering
        - This is a technique that tries to identify similat inputs in data and then puts them into clusters or categories
        - The goal is to find hidden patterns in data
    - K-Means Clustering
        - https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1
        - Firstly the hyperparameter k is set, it represents the number of clusters that K-Means Clustering will create once done
        - K random vectors (also called centroids) are picked in the feature space, they are changed during the training process
            - The goal is to put the into the centre of each clustering
        - The distance from each input sample x to each centroid c is calculated using some metric
            - This is usually Eucalidean distance, the closest centroidis assigned to each sample in the dataset and clusters are created
        - An average cluster feature vector is calculated for samples that are assigned to it
        - The previous steps are repeated for either a fixed number of iterations or until the centroids don't change
    - Elbow Method
        - https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/
        - This method is built on 2 metrics distortion and inertia
        - Distortion is calculated as the average of squared distances which can be Eucalidean distances from the cluster centres
        - Inertia on the other hand represents the sum of the squared distances of samples from the center of the closest cluster
        - The 'elbow' point is the after which distortion and inertia begin to decrease in linear fashion as cluster numbers grow
            - This point is the optimal number of clusters
    - Hierarchical Clustering
        - Each hierarchical clustering method begins by putting all samples into separate single sample clusters
        - After this based on similarity metrics clusters are merged together until the point where all clusters have become a single cluster
    - Agglomerative Clustering
        - Every point is stored in it's own cluster
        - The proximity matrix is then calculated
        - Closest points are then detected and merged, these are the cluster and the centroid is calculated
        - The proximity cluster is then updated using the centroid of the cluster that was created
        - The last 2 steps are repeated until there is 1 cluster created
    - DBSCAN
        - DBSCAN is not like the other clustering algorithms as it is density based
        - This means that there is not need to determine how many clusters are needed
        - There are 2 parameters defined, the distance and the number of samples per cluster
        - There are advantages to this approach as clusters have different shapes
            - Centroid based algorithms always create clusters that have the shape of a hypersphere
        - There is an optimised version of this algorithm called HDBSCAN

Clustering Notebook
    - Kmeans is the ML.NET implementation of K-Means Clustering
    - Penguins size is the dataset being used

Clustering Project
    - Again generate a cli project using dotnet
        - Also install the usual tools
    - The architecture is the same as other projects
        - A single trainer KMeans is used


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    AUTOML
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - AutoML is the process of automating the end to end process of applying machine learning 
    - https://www.automl.org/automl/
    - https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml

Model Builder
    - This is a visual studio extension
    - https://dotnet.microsoft.com/en-us/apps/machinelearning-ai/ml-dotnet/model-builder


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    RECOMMENDATION SYSTEMS
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - Recommendation Systems are used in lots of places
    - This type of system is one of the most successful applications in Machine Learning
    - They provide businesses with the ability to predict what users would like to buy, read etc
    - Users also benefit as they gain from increased product ranges to explore
    - Another benefit for businesses is increased user engagement.
    - There are different types of recommendation system
    - https://towardsdatascience.com/recommendation-system-part-1-use-of-collaborative-filtering-and-hybrid-collaborative-content-in-6137ba64ad58
    - https://towardsdatascience.com/recommendation-system-matrix-factorization-d61978660b4b
Content Based Systems
    - These systems collect data and present it tabularly and are item based
    - They calculate the similarity between items, other things can be calculated if required
    - Then a matrix list of items is created
    - The list is then sorted by similarity and the Top-N neighbours is picked
        -These will be items that highest similarity with users purchases
        - There may be a minimum similarity threshold used to avoid obscure recommendations
    - Items that have not been consumed by users
        - These are then weighted and ratings are normalised
Collaborative Filtering
    -These also collect data and present it in tabular form
        - Values for users and items are used (Ratings or consumption can be booleans)
    - Each user is represented as a multidimensional array
        - Similarity is calculated using only items that they have in common
    - A similarity matrix is then created, it must be born in mind though that if similarity is set to 1 does not necessarily mean that both users liked an item
        - They could have both just as easily disliked the same item equally
        - When data is sparse this can also distort to show 100% similarity
        - It may be a good idea to use a minimum threshold of shared items
    - The similarity list is then sorted and the top-N neighbours are picked
        - This means users have the most similarity with the selected items and a minimum similarity threshold
    - Then pick items that the selected users have not consumed 
        - This will be compared against a list of items that other similar users have liked
        - Suggest what other users have liked never what they hated
        - This is then weighted with the similarity score between users
Knowledge Based    
    - This sysyem uses explicit knowledge of a users prefernces or recommendation criteria
Hybrid
    - This is a system which incorporates parts from the 3 types of system listed above

Recommendation Systems Notebook
    - This will use a Netflix Prize dataset from Kaggle
    - There are several million records in the original so only a smaller sample is used
    - ML.Recommender will need to be installed
    - There is no need to use ToTabularDataResource to look at the various parts of the dataset
    - When creating the model the Recommendation trainer is used with Matrix factorisation
        - This takes several different parameters
        - labelColumnName for the label, learningRate, numberOfIterations for how many iterations will be used
        - matrixColumnIndexColumnName, matrixRowIndexColumnName

Recommendation Systems Project
    - This will be the same as other projects
    - Microsoft ML.Recommender will need to be installed, there is no need to install FastTree
    - MatrixFactorization will be the trainer that is used

Recommendation Systems ModelBuilder Project
    - https://dotnet.microsoft.com/en-us/apps/machinelearning-ai/ml-dotnet/model-builder
    - This will use the Visual Studio extension to build models without coding
        - Simply add a Machine Learning model to a project by creating a folder and right clicking and add Machine Learning model 
        - Once the mbconfig file has been created the window will show a number of options for adding a datset and performing predictions
        - There are numerous options available for running predictions
    - There is also an extension for VSCode in preview stage


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    IMAGE CLASSIFICATION
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - Image Classification is one of the most common computer vision problems
        - https://viso.ai/computer-vision/image-classification/
    - It is as the name suggests a classification problem
    - It differs from previous problems as images will be used as inputs
    - The goal remains the same, namely finding which class an input belongs to
    - Imagenet is the king of computer vision datasets
        - https://www.image-net.org/
        - This dataset is the benchmark and is used for new deep learning and computer vision breakthroughs
        - The world of deep learning would be different without it such is it's impact in shaping the world
        - It is a large image database organized using a WordNet hierarchy
            - https://wordnet.princeton.edu/
            - https://www.pythonstudio.us/language-processing/the-wordnet-hierarchy.html
            - This means that each image is described with a set of words and phrases called synset
        - For each synset approximately 1000 images is assigned
Neural Networks and Convolutional Neural Networks Intuition
    - https://www.ibm.com/cloud/learn/neural-networks
    - Previous projects have used shallow algorithms
        - They are called shallow because they learn model parameters directly from the features
    - https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9
Popular Image Classification Models
    - There are several architectures that won the ImageNet competition that became popular
        - Some of them are: VGG16, GoogLeNet (Inception), ResNet
    - These architectures are not only popular for transfer learning, some are used because they are easy to train
        - https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c
        - https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/
        - https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8
Transfer Learning
    - Training large models is computationally expensive, most people will not have the necessary equipment
    - Models can be trained in the clud but again will need large resources
    - This is where pre-trained models come in, this is simply a saved network that was trained on a large dataset
        - This can be used straight out of the box or fine-tuned
        - These large datasets are usually used for a larger more global solution, the model can be customised for a specific problem
        - This allows for using neural networks without using too many resources 
Image Classification Project
    - This project will use the Kaggle dataset for Dogs vs Cats
        - https://www.kaggle.com/competitions/dogs-vs-cats/data
    - ML.NET does not provide a way to build Neural Networks or Convolutional Neural Networks
    - It also does not provide popular algorithms
    - Tensorflow will be used, the .NET implementation to be more precise
        - https://github.com/SciSharp/TensorFlow.NET
    - There are some differences when installing from the cli rather than using Visual Studio
        - https://www.nuget.org/packages/TensorFlow.NET
    - The rest of the project is similar to previous projects
        - There is a predictor file
        - There is however no need for a trainer file as this will be done using a data loader and classifier file


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    OBJECT DETECTION
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - Object detection is a more complex problem than Image Classification
        - The output of an object detection solution is not just the class to which the object belongs
        - Object Detection systems are able to detect where an object is in an image and draw a bounding box around it.
        - They do also provide predictions on the class of an object
    - YOLO -- You Only Look Once
        - https://towardsdatascience.com/how-to-train-a-custom-object-detection-model-with-yolo-v5-917e9ce13208
        - YOLO is a Convolutional Network the simultaneously predicts multiple bounding boxes and the probabilities of the classes of these boxes
        - YOLO is based on regression instead of selecting a particular part of an image
        - It predicts classes and bounding boxes in one run
        - Using YOLO the input image is divided into a grid measuring S x S
        - If the object is at the centre of the grid cell then that particular grid cell should detect the object
        - This is done by predicting B number of bounding boxes and confidence that it is within that grid cell
        - Each bounding box is defined as a Tuple containing x, y, w, h, confidence
            - w, h are width and height while x and y are the coordinates of the centre
YOLO History
    - The above shows how the core principles of how YOLO work
    - This first version is extended over the years with new concepts and changes in the architecture
    - Core Principles remain but each version bought improvements
    - YOLO v2
        - This version introduced many features for which YOLO is known and loved
        - It brought performance improvements, introduced anchors and multi-scale training
        - The architecture is trained on a combination of ImageNet and COCO dataset so it is able to recognize 9000 classes of objects
        - The most noticeable change however is the introduction of Anchor boxes
        - Older concepts like Faster R-CNN used the concept of pre-given anchor boxes to predict bounding boxes for objects
            - This essentially means that the regression used by YOLO v1 was not used
        - Instead 3 different scales and 3 different aspect ratios were used to predict offsets for the pre-given anchor boxes.
            - Boxes were then predicted using that offset
        - This way the algorithm needs to learn offset and selected size, and it doesn’t need to learn coordinates and dimensions of the bounding box
        - YOLO v2 goes further it does not use predefined anchor boxes, instead it uses bounding boxes of training data
        - K-Means Clustering is then ran on them which then picks a set of dimension clusters that are applicable
        - In YOLO v2 multi-scale has been introduced
            - This means that the network is randomly resized during the training process in the multiples of 32
            - This has had a positive effect on the performance of YOLO v2
        - WordTree was also introduced which is a specially tailored dataset using a combination of COCO and ImageNet
            - Combining these two datasets together, a tree structure is implemented with hierarchies like wordnet
            - Instead of having a single SoftMax deciding which class is in the image, the whole tree is used
    - YOLO v3
        - Under this version YOLO became the most popular architecture for object detection
        - It mainly focused on improving existing concepts
            - YOLOv3 predicts 10x more bounding boxes than YOLOv2 in 3 different scales
            - Instead of SoftMax YOLOv3  uses independent logistic classifiers with binary cross-entropy loss
            - YOLOv3 uses a new convolutional neural network with 53 layers (Darknet-53) for feature extraction
    YOLO v4 and 5
        - YOLO v4 is a fork of the original project due to work being stopped by the original author
        - This version introduces two terms Bag of freebies (BOF) and Bag of specials (BOS)
            - Bag of freebies refers to the methods that affect training strategy
                - One such method is data augmentation, which is used to increase the variability of the input images and make the model has higher robustness
            - Bag of specials is post-processing modules and methods that do increase the inference cost but improve the accuracy of object detection as well
        - YOLO v5 is a completely different version than predecessors
            - It uses a PyTorch implementation rather than the original Darknet architecture


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    SENTIMENT ANALYSIS
--------------------------------------------------------------------------------------------------------------------------------------------------
Sentiment Analysis Intuition
    - It is used to monitor and understand customer feedback
    - It detects the underlying emotional tone of words and derives understanding customer needs
    - It helps provide better products and services
    - It can be positive, negative or neutral
    - There are different types of Sentiment Analysis
        - Emotions, Fine Grained feedback, Intent Analysis, Aspect-based
Sentiment Analysis Notebook
    - Need to install Recommender and ExtensionLab
    - Dataset being used is a labelled IMDB dataset 
Sentiment Analysis Project
    - Reuses the template from previous projects
    - The IMDB labelled dataset is also used here
    - There are multiple trainer used for the project
        - GAM, Decision Tree, Random Forest among the examples


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    NLP WITH BERT
--------------------------------------------------------------------------------------------------------------------------------------------------
NLP Intuition
    - NLP or Natural Language Processing is a field of machine learning and linguistics focused on understanding everything to do with human language
    - The aim of NLP tasks is to not alone understand single words individually but also understand the context they are being used.
    - There are multiple different NLP tasks
        - Text Classification
        - Words Classification such as Named Entity Recognition
        - Text Completion in sentences
        - Extracting answers from text
        - Generating new text such as translation and summarisation 
    - There are huge challenges in the world of NLP
        - This is because computers understand numbers and not words
        - Text analysis is very complex because extracting meaning and context from text is difficult because language itself is difficult
    - https://monkeylearn.com/blog/nlp-ai/
Transformers Intuition
    - Transformers are a network architecture based solely on attention mechanisms
    - These architectures dispense with recurrence and convolutions completely
    - Experiments have shown these models o be higher quality as well as being quicker to train and easier to parallelize
    - They are primarily language models that have been trained on very large amounts of text data
    - There are various options with them so that has led to their use in Deep Learning 
    - They bought transfer learning into the NLP field
    - High Level architecture
        - Encoder
            - This recieves an input and then builds a representation of it
            - This means that the model is optimised to acquire understanding from the input
        - Decoder
            - This uses encoder representations as well as other inputs
            - This is then used to generate a target sequence
            - This means that the model is optimised for generating outputs
    - Attention
        - A key feature of transformers is that they are built using special layers called attention layers
        - These layers tell the model to pay specific attention to certain words passed in a sentences
        - Other words can be more or less ignored
    - Three types
        - Encoder Only Models
            - These are good for tasks that require input understanding such as sentence classification and named entity recognition
        - Decoder Only Models
            - They are good for text generation
        - Encode-Decoder
            - These are also known as sequence to sequence models
            - They are good for generative tasks such as translation and summarisation
    - Encoder Models
        - These are bi-directional which makes them good at extracting context and useful information from text
        - They are used a lot in NLU or Natural Language understanding
        - Some of the uses are sequence classification, questions and answers, masked language modelling
        - Some examples are BERT, RoBERTa and other variations  
    - Decoder Models
        - These are uni-directional which is why they excel at generating text when given the context
        - Masked self-attention layers are used to hide the context
        - They make use of an AOT regressive mechanism which means using their own output as input
        - They excel at NLG or Natural Language Generation
        - Some examples of decoder models are GPT-2, GPT-3
    - Sequence to Sequence
        - This model uses a separation of responsibilities
        - The encoder understands the sequence and context
        - The deocder uses that understanding to generate new sequences
        - This is why they are used for text translation tasks and summarisation
        - Examples of sequence to sequence architecture are BART and T5
Architecture    
    - https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/
    - https://towardsdatascience.com/transformers-89034557de14
    - https://towardsdatascience.com/intuition-behind-transformers-architecture-nlp-c2ac36174047
HugginFace Models
    - https://huggingface.co/course/chapter1/1
    - https://neptune.ai/blog/natural-language-processing-with-hugging-face-and-transformers
NLP with BERT Project
    - Once again a simple console application will suffice
    - The model will need to be downloaded
        - https://github.com/onnx/models/tree/main/text/machine_comprehension/bert-squad/model
        - Bertsquad 10 is the model needed
        - Add to gitignore file to stop it being pushed during commits
    - Both Onnx packages will need to be installed as well as the BertTokeniser package
    - There will also be an extensions folder
        - This will hold enumerables which will perform soft-max separations on enumerable types
    - There will only be one sub-folder in machine learning folder in this project
        - This will be to hold the data models
    - BERT inputs are much more strict in C# because of the fact that it is strongly typed
        - These dimensions are set in the BertInput data model file
    - Using the GPU in the trainer can be done by using the gpuDeviceId directive
        - In this project it can be set to 0 as there is no need to use a GPU
    - Text to check whether the prediction works can be added as command line options

--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    TROUBLESHOOTING
--------------------------------------------------------------------------------------------------------------------------------------------------
Running C# code from CLI in VSCode
    - https://docs.microsoft.com/en-us/dotnet/core/tutorials/with-visual-studio-code?pivots=dotnet-6-0
    - https://www.youtube.com/watch?v=CO4BGZOuUkM&t=435s

Install ML.NET into a project
    - https://docs.microsoft.com/en-us/dotnet/machine-learning/how-to-guides/install-ml-net-cli?tabs=windows
    - dotnet tool install --global mlnet-win-x64
    - Confirm installation has worked by the following command
        - mlnet

Install MathNet package into project
    - https://www.nuget.org/packages/MathNet.Numerics/
    - Using the .NET_CLI -- dotnet add package Mathnet.Numerics
    - Versions can be specified but if a version is not specified then the latest is installed

Running cells
    - This can be done using either the play buuton or shift+enter

Naming Training files
    - This has caused some issues with some files name clashing with ML library files
    - I have added U into the filename to indicate that this is a user generated file

Copying files
    - Ensure that namespaces are changed and that datasets are the same !!!

Moving Files
    - Moving files in VSCode may cause issues in projects
    - It maybe better to copy the code and create a new file in the necessary folder

Gitignore Files
    - https://www.pluralsight.com/guides/how-to-use-gitignore-file
    - https://github.com/github/gitignore

Git issues
    - Sometimes time commits can get out of alignment
        - git reset -hard origin/<branch name>
    - Trying to push too much can cause an error, increase buffer size should help
        - https://stackoverflow.com/questions/59282476/error-rpc-failed-curl-92-http-2-stream-0-was-not-closed-cleanly-protocol-erro
        - git config --global http.postBuffer 524288000
