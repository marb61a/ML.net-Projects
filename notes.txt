Notes from Rubikscode ML.net course
    - https://rubikscode.net/courses/ml-net-full-stack-machine-learning-for-net-developers/
    - Machine Learning for .Net developers
    - https://docs.microsoft.com/en-us/dotnet/machine-learning/
    - https://github.com/dotnet/machinelearning
    - I have started using NB prefix for projects when using interactive notebooks

Complete suite of .NET versions of ML\AI libraries
    - https://scisharp.github.io/SciSharp/
    - https://github.com/SciSharp
    - https://github.com/dotnet/TorchSharp

Nimbus ML provides python bindings for ML.NET
    - https://docs.microsoft.com/en-us/nimbusml/overview
    - https://github.com/microsoft/NimbusML

Using Model Builder in VSCode
    - https://www.youtube.com/watch?v=R8aCkjSTSuc

I am very interested in ML.net as .net is used in RPA tools which gives some benefits
    - Learning C# based machine learning which can be used not only in RPA tools
    - https://docs.uipath.com/ai-fabric/v0/docs/building-ml-packages

There are some pre-requisites for the course
    - VSCode and Visual Studio 2022
        - .net 6 comes with the IDE installation
    - .net Interactive VSCode extension
    - The notebooks involved are similar to Jupyter notebooks
    - C# will be used instead of Python but will be used in an almost script-like manner

There are some datasets that will be used within the course
    - Palmer Penguins Dataset, Boston Housing Dataset, Netflix Prize Dataset, IMDb Sentiment Dataset and Cats vs Dogs

History of Machine Learning
    - https://www.techtarget.com/whatis/A-Timeline-of-Machine-Learning-History
    - https://www.dataversity.net/a-brief-history-of-machine-learning/
    - Machine Learning is no longer an obscure topic
    - Data volumes have increased by orders of magnitude in the last few years
        - By 2025 expected to be around 200 Zettabytes
        - A zettabyte is 1,000,000,000,000,000,000,000 or 10 to the power of 21
    - Machine Learning and Deep Learning are very in demand skills
    - A quick definition of Machine Learning
        - Machine Learning is the branch of computer science which uses statistical techniques to give computers the ability to learn
    - Learning in this context means gaining the ability to perform tasks to success rates through experience
    - Machine Learning tries to solves problems that are tough to solve by people or standard software
        - Problems include classification, regression, forecasting and anomaly detection

Gradient Descent
    - https://en.wikipedia.org/wiki/Gradient_descent
    - https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
    - Gradient Descent is the essential optimization technique and one of the main ingredients of some algorithms
        - It is the most popular optimisation technique and the basis for all others

Performance Metrics
    - Evaluation metrics are in general grouped by the task being solved by an algorithm
        - For example binary classification uses a different set of metrics to determine algorithm performance than would be used in regression
    - https://neptune.ai/blog/evaluation-metrics-binary-classification
    - There are some terms to consider when assessing classification model performance
        - Confusion Matrix or Error Matrix
            - https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/
            - Used to describe classification model performance when true values are known
        - Precision is another technique
            - It also carries more information than accuracy
            - It asks the question about what proportion of positive identications were correct
            - The value goes from 0 to 1 and the closer to 1 the better
        - Recall which is described as the ability of the classifier to find all the positive samples
            - This metric tries to answer the question, What proportion of actual positives was identified correctly
            - It is very similar to precision but differs slightly as precision measure result relevance
            - Recall on the other hand measures how many truly relevant results are returned
        - Accuracy is calculated as a number of correct predictions divided by the total number of predictions
            - This can be trick as it may lead to incorrect impressions about the model especiall when using imbalanced data
            - This means thet if the model performs well on the dominant class in the dataset it may not perform as well on others
            - Models that overfit have 100% accuracy
        - F1 Score is probably the most popular metric that combines precision and recall
            - It represents the harmonic mean of both
            - Its formula is (Precision * Recall / Precision + Recall) * 2 
        - Receiver Operating Characteristic (ROC) curve & Area Under the curve (AUC)
            - When an ML algorithm is predicting the class of a sample it calculates the probability that it belongs to a certain class
            - If the value is above a certain threshold then it is label as that class
            - ROC curves show the true positive rate against the false positive rate for certain thresholds 
            - The AUC metric is used as a measure of performance
            - This means that ROC is a probability curve where AUC measures the separability
            - This combination tells the model to  distinguish classes and the higher the value the better
        - Area under Precision-Recall Curve (AUPRC)
            - In order to correctly evaluate a model, both metrics need to be taken into consideration
            - Unfortunately as precision improves it usually reduces recall and the opposite is also true
            - The precision-recall curve shows the tradeoff between precision and recall
            - Areas under the curve represents both high recall and high precision
            - High scores for both show that the classifier is returning accurate results with a majority of positive results
    - Multi-Class Classification Metrics
        - Using multi-class classification we can use some of the same metrics that we use for binary classification
        - There are some differences for multi-class classification though
        - Confusion Matric, the difference when being used with multi-class is that each class has a row
        - Micro and Macro Accuracy
            - An accuracy formula can be applied for binary classification for eash class in a dataset
            - There are 2 different ways, the first is when all classes are treated equally, the metric is computed independently for each class and the average is taken
            - This first method is called Macro accuracy, the second is called Micro Accuracy
            - This is when we aggregate the contributions of all classes to compute the average metric
            - Micro Accuracy is a more reliable metric as it tries to account for any class imbalance
        - Log-Loss & Log-Loss Reduction
            - This is one of the most commonly used metrics in kaggle competitions
            - Log Loss is a metric that quantifies the accuracy of a classifier by penalizing false classifications
            - This metric’s value represents the amount of uncertainty of our prediction based on how much it varies from the actual label
            - Log-Loss Reduction is also called reduction in information gain – RIG
            - It gives a measure of how much a model improves on a model that gives random predictions
            - The close that RIG is to 1 the better the model
    - Regression Metrics 
        - Goals from regression problems differ to classification problems so different metrics are needed
        - Regression output is always continuous and the metrics need to match this
        - Mean Absolute Error – MAE
            - This calculates the average absolute distance (error) between predicted and targeted values
            - The closer to 0 the better, this also deals with outliers fairly well
        - Mean Squared Error – MSE
            - This is probably the most popular metric of all regression metrics
            - This calculates the average squared distance (error) between predicted and targeted values
            - Results are non-negative values and the goal is to get this value as close to zero as possible
            - This function is often used as a loss function of a machine learning model
        - Root Mean Squared Error – RMSE
            - This is a variation of the MSE metric
            - It shows what is the average deviation in predictions from the actual values
            - It makes and assumption that errors are unbiased and follows normal distribution 
            - RMSE is also a non-negative value and ) is the ideal value to be achieved
            - Lower RMSE values are better than higher
            - Outliers do affect this metric so a dataset must have them removed prior to using this metric
        - R Squared
            - R Squared is used when a more intuitive approach than MSE or RMSE is needed
            - It is also known as the coefficient of determination and is very popular
            - https://en.wikipedia.org/wiki/Coefficient_of_determination

How does ML.NET fit
    - ML.NET is Microsoft's ML framework which allows for model running, training etc within the .net ecosystem
    - ML.NET is faster that Pytorch using CPU
    - ML.NET was designed to be intuitive for .NET developers
    - The DataView class borrows from database concepts and is the reason why ML.NET is fast
    - ML.NEt performance on the GPU is not as good as Pytorch
    - Data Preprocession can be complicated
    - There not that many tools for data visualisation


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                LINEAR REGRESSION
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - https://machinelearningmastery.com/linear-regression-for-machine-learning/
    - The output of solving a regression problem is a continuous numerical value
        - One of the most famous is predicting the price of a house based on tenants ages which are independent variables
    - There are multiple sources for learning about the mathematics involved in linear regression
        - https://www.analyticsvidhya.com/blog/2021/08/understanding-linear-regression-with-mathematical-insights/
    - Multiple linear regression uses the reknowned Boston housing dataset
        - https://corporatefinanceinstitute.com/resources/knowledge/other/multiple-linear-regression/

Using Notebooks for Linear Regression
    - There are several steps needed
    - Install packages (NNuGet) and add using directives
    - Load and split the data
    - Train the model
    - Evaluate the model
    - Save the model
    - Load and predict

ML.NET Linear Regression
    - Online Gradient Descent
        - This is a variation of Stochastic Gradient Descent
        - It has a choice of loss functions and an option to update the weight vector which uses the vector average seen over time
    - SDCA which stands for Stochastic Dual Coordinate Ascent
        - It is another variation on Stochastic  Gradient Descent
        - This is suitable for large datasets as it can be scaled easily

Using C# Notebooks in VSCode
    - Files will still have the .ipynb extension
    - The kernel that will be used though is .NET interactive rather than a Python kernel
    - When installing packages use #r and then package name
        - In python notebooks this is done with an !
    - When loading a csv file
        - Use LoadFromTextFile<> which is part of MLContext.Data
        - This takes a couple of parameters, the name of the class (type) between the <>
        - The file path and whether the file has headers in the following brackets
            - Using the hasHeader directive which is set to true or false
        - Then the file delimiter is added using the separatorChar directive followed in this case by a comma
    - Splitting a dataset into testing and training
        - Again use MLContext.Data this time adding the TrainTestSplit method
        - The first parameter is the dataview created when loading the data
        - The second parameter is the amount of data to be used for testing using the testFraction directive which is set to 0.2 which equals 20%
        - The training dataset can be examined by using the variable that is assigned to the split and adding the TrainSet method for the training data
        - Then chain the ToTabularDataResource method on
        !!!! - Using a semi-colon will prevent the table from displaying
        - The test portion of the dataset can be loaded similar to above but using the TestSet method
    - Training the model
        - This uses the MLContext followed by Regression which then has trainers and finally the algorithm which is being used
        - In this case it is the SDCA algorithm being used to train the data
        - This takes 2 parameters which are label column name and feature
            - A feature column is added using the featureColumnName which is then given a value
            - A label column is defined using the labelColumnName directive which is then given a value
        - The dataset will need to be normalised prior to training
            - Training is a process which puts everything into the same scale
            - Differing scales can cause issues when training
            - The rivercoast feature is categorical which is a discrete\categorical value
        - We can create a piepline where all of the needed normalisation can be done
            - These can be done with MLContext Transforms
            - To transform categorical data use OneHotEncoding
                - One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms
                - https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f
            - After categorical data has been encoded then each of the features can be added using the Concatenate method available to Transforms
            - Then it is time for normalisation this time using NormalizeLpNorm
                - This takes 2 parameters which are source and destination, in this case "Features" for both
        - Once the pipeline has been set up then it can be used to train the model
            - The fit method is called then to train the training portion of the dataset
    - Evaluate the model
        - After the model has been trained then it is time for its performance to be evaluated
        - Using the trainedModel and Transform which takes the dataset TestSet
        - Evaluation can then be done with MLContext Regression and the Evaluate method which takes the variable that the transform operations have been assigned to
        - These metrics are MSE, MAE, RMSE, R Squared and Loss function
    - Save the model
        - Once the model has been trained it is time to save it for further use into the future
        - This is done using MLContext.Model.Save and takes some parameters strating with the trainedModel
        - The next parameter is the data schema which is needed, in this case the training data schema
        - The final parameter is the location the model will be saved, the model will always need a .mdl file extension
    - Load the model and Predict
        - This loads the model from the created file and can be used for predictions
        - The first step is to create a new sample which can be used to see if the model will predict correctly
        - In this case the first sample from the boston dataset serves well
        - Then the model is loaded using filestream and a using statement
        - The various file methods can then be used, FileMode set to open, FileAccess set to read and the same for FileAccess
        - This allows for opening the file, reading it and then again subsequently
        - Then load the model using MLContext.Model.Load
        - After the model has been loaded then the prediction engine needs to be created once again using methods from the model catalogue
        - Once the prediction engine has been put in place it can then be used with the prediction method and the new sample for a prediction


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                CLASSIFICATION
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623
    - The section will cover both binary and multiclass classification using both notebooks and normal projects
    - Classification algorithms build decision boundaries implicitly or explicitly
        - These boundaries can be straight, curved or have complex forms
        - The form of the boundary will determine the accuracy of the model
    - Classification works with discrete values, these are not continuous like regression
        - A person either is a gender or not, speaks a language or not etc
Binary classification notebook
    - This uses the penguins size dataset
    - The same packages as regression will need to be installed
    - Again the .NET interactive kernel is used
        - If not set then python will try to run those cells which cause errors
    - The same using directives are also needed at this point
    - Not all features will be used in the binary classification notebook example
    - After creating the class for holding the features that are needed another class is added for predictions
        - This will have a boolean method as classification is inherently true or false
    - Then it is time to import the data
        - Again as previously MLContext is used, this is used with the LoadFromTextFile method
        - This is a template function so the type wanted needs to be added which is the first class created
        - The parameters are then added starting with the data location and then whether there are headers and the separating character
    - After the data has be imported then it is time to split the data into test and train portions
        - In this example this will be 0.3 or 30%
        - This can be viewed using the ToTabularDataResource method again remebering to have no semi-colon to ensure rendering of table
    - The algorithm that will be used in modelling in this section is LBFGS
        - It is a variation on the the logistic regression that is based on the limited memory Broyden-Fletcher-Goldfarb-Shanno hence L-BFGS
        - https://en.wikipedia.org/wiki/Limited-memory_BFGS
    - The model will use MLContext.BinaryClassification.Trainers.LbfgsLogisticRegression
        - The name of the features column and label column will need to be set here as it was in the regression example
    - After stting up the model then it is time to construct a pipeline
        - This follows much tthe same process as seen previously
        - Again the data needs to be normaised this time using the NormalizeMinMax function
        - This is based on using the minimum and maximum values as a guide
        - There is also a need for source and destination parameters (Both can be set to "features")
        - Finally append the model to the pipeline
    - After setting up the pipeline then it is time to train the model
        - This is done using the Fit method
    - Then it is time to evaluate, first the test portion of the dataset needs to be transformed
        - Then this transformed data is passed to an Evaluation function which is part of the MLContext.BinaryClassification catalogue
        - The accuracy can be seen by printing out the metrics result and using its Accuracy method
    - After evaulation the result is about 96%
        - This means that it is good enought to be saved to disk
        - This saving and reloading will be the same as for the regression project

Standard C# Binary Classification
    - The projects that are using standard C# development techniques will not have a prefix, notebook projects begin with NB
        - There can be a standard console application used for this project
    - Firstly install a console application into the folder using the following command 
        - dotnet new console --framework net6.0
    - Once that is done then add in the data which will be in a data folder, again the PalmerPenguins dataset will be used
    - Then it is time to start adding in files and folders first up is a usings file to list all the using statements in the project
        - Each statement will be accompanied by a global keyword to make the usings available to all files
        - https://www.c-sharpcorner.com/article/global-using-directive-in-c-sharp-102/
    - Then a MachineLearning folder will be added, it will have several subfolders
        - DataModels which have 2 files, one is where we setup the type using LoadColumns to setup the appropriate structure
        - The second file will be the file that is used when predicting, it only needs a single variable
    - Then there is Common folder which contains the ITrainerBase class 
        - C# interfaces contain declarations of methods, properties, indexers, and events
        - https://www.tutorialsteacher.com/csharp/csharp-interface
    - After the interface comes the implementation class, this time called TrainerBase
        - This class is public and will need to use the items from the interface
        - https://www.w3schools.com/cs/cs_interface.php
        - It will need to implement all items from the interface to avoid errors
    -  This project makes use of several trainers including the LbfgsLogisticRegression
        - Others are AveragedPerceptronTrainer
        - PriorTrainer
        - SDCALogisticRegressionTrainer
        - SDCANonCalibratedTrainer
        - SGDCalibratedTrainer
        - SGDNonCalibratedTrainer

Multiclass Classification notebook
    - There are different algorithms support when running multiclass classification
        - LBFGS Maximum Entropy
            - This maximum entropy model differs from logistic regression in the number of classes supported
            - Logistic regression is used for binary classification whereas the maximum entropy handles multiple classes
            - It is also based on the Broyden-Fletcher-Goldfarb-Shanno method
        - Naive Bayes
        - One versus all
            - This performs binary classification for each class of the dataset and then creates multiple binary classifiers
            - Predictions are then made by running binary classifiers against each other to find the one with the highest confidence score
        - SDCA Maximum Entropy
            - This is also based on logistic regression
        - SDCA Non-Calibrated which is the uncalibrated version of the above algorithm
    - This uses a slightly different version of the penguins dataset
        - There are no 0 and 1 values for species
        - This is the full dataset rather than the subset previously used
    - This dataset does not have the species column which means the there will  need to be a conversion of the first column
        - This column is the species label but needs to be in numerical form when building the pipeline
        - This is handled by using mlContext.Transforms.Conversion.MapValueToKey() where MapValueToKey maps the text value to a numerical value
        - This takes 2 parameters an input column name and an output column name

Multiclass Classification Project (Non-notebook)
    - This is also very similar to the binary classification project
    - Again use the following command to create a new console project
        - dotnet new console --framework net6.0  
        - Manifests and packages will have to be installed
            - dotnet new tool-manifest
            - dotnet add package Microsoft.ML 
            - dotnet add package MathNet.Numerics
    - Subtle differences only such as the Evalute methods
        - In the binary classification used mlContext.BinaryClassification.EvaluateNonCalibrated
        - In the multiclass classification it was mlContext.MulticlassClassification.Evaluate
        - very large parts of the project was the same except changing names to multiclass instead of binary


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                DECISION TREES
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - https://en.wikipedia.org/wiki/Decision_tree_learning
    - https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/
    - There are multiple algorithms available but CART is covered here
    - Impurity is defined as a chance of being incorrect if you assign a label to an example at random  
        - A node is pure if all training instances it applies to belong to the same class
        - You therefore cannot makke a mistake when assigning a label to a random sample
        - https://www.learndatasci.com/glossary/gini-impurity/

Decision Tree Classification Notebooks
    - There are several decision tree classification algorithms supported by ML.NET
        - Fast Tree which is an implementation of the MART algorithm which stands for Multiple Additive Regression Trees
            - This algorithm has been known to deliver high prediction accuracy and is widely used
            - MART is an ensemble model of boosted regression trees, which means using gradient boost for calculations
            - It builds each regression tree in a stepwise fashion using a loss function to measure the error for each step which is corrected in the next
            - In regression boosing builds a series of trees step wise and selects the optimal tree
            - https://analyticsindiamag.com/a-beginners-guide-to-bayesian-additive-regression-trees/
        - Fast Tree Tweedie
            - This is at it's heart similar to Fast Tree but it uses a different gradient boosting algorithm
        - GAM or Generative Additive Models are usually implemented with decision trees 
            - It treats data as a set of linearly independent features
            - https://en.wikipedia.org/wiki/Generalized_additive_model
    - Needs the Microsoft.ML.FastTree library installed
        -A lot of the notebook from the binary classification notbook can be reused
    - When training the model the FastTree trainer is used and takes several parameters
        - The first parameter is the number of leaves in this case it is set to 3
        - The second parameter is the number of trees in this case it is also set to 3
        - The third parameter is the learning rate which determines how fast a model learns
            - https://en.wikipedia.org/wiki/Learning_rate

Decision Tree Classification
    - Again this follow the previous projects ways of doing things
    - Start off by creating a new directory and creating a console project inside it
    - There needs to be a separate install ation of the FastTree package
        - dotnet add package Microsoft.ML.FastTree
        - https://rubikscode.net/2021/04/26/machine-learning-with-ml-net-sentiment-analysis/
        - https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.trainers.fasttree?view=ml-dotnet

Decision Tree Regression Notebook
    - This will use the fasttree algorithm in the context of a regression problem
    - Installation packages are the same as for classification
    - Dataset used is the boston housing dataset
    - This will be very similar to the LinearRegression project
    - When normalizing the data the NormalizeLogMeanVariance normaliser is used
        - https://towardsdatascience.com/how-to-differentiate-between-scaling-normalization-and-log-transformations-69873d365a94
    - Increasing the number of trees when training the model will impace on the final prediction
        - 18.5 with 3 trees compared to 27.98 for 10 trees 

Decision Tree Regression
    - This will use the fasttree algorithm in the context of a regression problem
    - The FastTree package will need to be installed separately in the project
    - The boston house prices dataset will be used
    - One of the trainer files uses the FastTree Tweedie algorithm
    - Again had to rename the training file by adding a u to indicate user made
        - This is to prevent any errors due to ambiguity where file names are the same as library file names


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    RANDOM FOREST
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - Ensemble Learning
        - Sometimes in ML better results can be gotten by using multiple predictors and averaging results rather than one algorithm
        - Using multiple algorithms is referred to as ensemble learning
        - https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/
        - https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f
        - Ensemble Learning is based on the law of large numbers
            - https://en.wikipedia.org/wiki/Law_of_large_numbers
    - Hard Voting
        - Hard Voting is a type of ensemble learning
        - https://en.wikipedia.org/wiki/Law_of_large_numbers
        - If we take several classifiers voting for a class, the class that gets the majority of votes is the output
        - An ensemble containing 1000 classifiers, each has 51% accuracy alone, using ensemble hard voting can have accuracy of 75%
    - Soft Voting
        - Each of the algorithms will output a probability
        - The ensemble will predict the class with the highest class probability averaged over each of the classifiers
    - Bagging and Pasting
        - https://medium.com/@silvaan/ensemble-methods-bagging-and-pasting-in-scikit-learn-723f4183cdf4
        - This is where the same algorithm is used on different subsets of the training dataset
        - It is one of the most popular ways to build ensembles
        - There is only one major difference between bagging and pasting
            - Bagging allows training instances to be sampled several times for the same predictor where pasting does not
        - When all algorithms are trained the ensemble makes a prediction by aggregating the predictions of all algorithms
    - Feature Bagging
        - This is where during training each tree is trained on a subset of features
        - This in turn will lead to lower variance of the complete model

Random Forest Classification Notebook
    - This will cover Random Forest algorithms for classification
    - ML.NET supports Random Forest for both regression and classification
    - Random Forest classification is limited to binary classification
    - The Random Forest algorithm is called Fast Forest and is an ensemble build of Fast Tree
    - This n will reuse the penguins binary dataset
    - When training the model using FastForest there is no Learning_rate specified

Random Forest Classification Project
    - Again this will be a command line project using the DOTNET cli
    - The dataset being used will be the penguis binary dataset
    - The only trainer will be a Random Forest trainer file

Random Forest Regression Notebook
    - This time Random Forest will be assed for regression purposes
    - The Boston housing dataset will be used
    - When training the number of leaves is set to 30 and number of trees to 100
    - Once again with FastForest a learning rate is not needed

Random Forest Regression Project
    - Using the Boston housing dataset
    - Similar to both the classification project and other regression projects


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    SUPPORT VECTOR MACHINE
--------------------------------------------------------------------------------------------------------------------------------------------------
Intuition
    - https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47
    - SVM is one of the most popular ML algorithms
    - SVM observes every feature vecor as a point in a high dimensional space
    - It puts all feature vectors on an imaginary n-dimensional plot
    - It then draws an imaginary n-dimensionl line called a hyperplane
    - This separates examples with positive labels from negative labelled examples when dealing with classification
    - When dealing with regression it collects as much sampling as possible
        - https://towardsdatascience.com/unlocking-the-true-power-of-support-vector-regression-847fd123a4a0
    - SVM does not only create a hyperplane
        - There are also additional vectors constructed which define the margin
        - These vectors are called support vectors and the difference between the 2 closest classes is called the margin
        - Hyperplanes with support vectors is often called street
        - https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496
    - Kernel trick
        - This technique gives you the possibility to get the same result as if polynomial features were being used without using them
        - Kernels are just functions that map low-dimensional non-linearly seperable data into the opposite (high-dimensional linearly-separable data)
        - One example is mapping 2d non-linearly separable data into 3d separable data
        - https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f

Support Vector Machine Notebook
    - Linear SVM
        - This is an ML.NET implementation of the PEGASOS algorithm
        - https://qiskit.org/documentation/machine-learning/tutorials/07_pegasos_qsvc.html
    - Local Deep SVM   
        - This implementation is a generalisation of Localised Multiple Kernel Learning for non-linear SVM
        - LD-SVM speeds sup non-linear SVM which tend to be slower
        - Non-linear SVM uses kernel learning

Support Vector Machine Project
    - Again this will be a cli project
    - It will be a classification project
    - The architecture will be the same as other project folders
    - The trainers in this project will be both LinearSVM and LdSVM


--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    CLUSTERING
--------------------------------------------------------------------------------------------------------------------------------------------------
This lesson is where the shift away from supervised learning begins
    - There will be 3 clustering algorithms covered,  K-Means Clustering, Agglomerative Clustering and DBSCAN
Intuition
    - Unsupervised Learning
        - In real life there is only input data available
        - This means that the algorithms that we use need to figure out any connections between input samples
        - This is where unsupervised learning comes in
        - Supervised learning can solve regression and classification problems but unsupervised learning solves clustering problems
    - Clustering
        - This is a technique that tries to identify similat inputs in data and then puts them into clusters or categories
        - The goal is to find hidden patterns in data
    - K-Means Clustering
        - https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1
        - Firstly the hyperparameter k is set, it represents the number of clusters that K-Means Clustering will create once done
        - K random vectors (also called centroids) are picked in the feature space, they are changed during the training process
            - The goal is to put the into the centre of each clustering
        - The distance from each input sample x to each centroid c is calculated using some metric
            - This is usually Eucalidean distance, the closest centroidis assigned to each sample in the dataset and clusters are created
        - An average cluster feature vector is calculated for samples that are assigned to it
        - The previous steps are repeated for either a fixed number of iterations or until the centroids don't change
    - Elbow Method
        - https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/
        - This method is built on 2 metrics distortion and inertia
        - Distortion is calculated as the average of squared distances which can be Eucalidean distances from the cluster centres
        - Inertia on the other hand represents the sum of the squared distances of samples from the center of the closest cluster
        - The 'elbow' point is the after which distortion and inertia begin to decrease in linear fashion as cluster numbers grow
            - This point is the optimal number of clusters
    - Hierarchical Clustering
        - Each hierarchical clustering method begins by putting all samples into separate single sample clusters
        - After this based on similarity metrics clusters are merged together until the point where all clusters have become a single cluster
    - Agglomerative Clustering
        - Every point is stored in it's own cluster
        - The proximity matrix is then calculated
        - Closest points are then detected and merged, these are the cluster and the centroid is calculated
        - The proximity cluster is then updated using the centroid of the cluster that was created
        - The last 2 steps are repeated until there is 1 cluster created
    - DBSCAN
        - DBSCAN is not like the other clustering algorithms as it is density based
        - This means that there is not need to determine how many clusters are needed
        - There are 2 parameters defined, the distance and the number of samples per cluster
        - There are advantages to this approach as clusters have different shapes
            - Centroid based algorithms always create clusters that have the shape of a hypersphere
        - There is an optimised version of this algorithm called HDBSCAN

Clustering Notebook
    - 

--------------------------------------------------------------------------------------------------------------------------------------------------
                                                    TROUBLESHOOTING
--------------------------------------------------------------------------------------------------------------------------------------------------
Running C# code from CLI in VSCode
    - https://docs.microsoft.com/en-us/dotnet/core/tutorials/with-visual-studio-code?pivots=dotnet-6-0
    - https://www.youtube.com/watch?v=CO4BGZOuUkM&t=435s

Install ML.NET into a project
    - https://docs.microsoft.com/en-us/dotnet/machine-learning/how-to-guides/install-ml-net-cli?tabs=windows
    - dotnet tool install --global mlnet-win-x64
    - Confirm installation has worked by the following command
        - mlnet

Install MathNet package into project
    - https://www.nuget.org/packages/MathNet.Numerics/
    - Using the .NET_CLI -- dotnet add package Mathnet.Numerics
    - Versions can be specified but if a version is not specified then the latest is installed

Running cells
    - This can be done using either the play buuton or shift+enter

Naming Training files
    - This has caused some issues with some files name clashing with ML library files
    - I have added U into the filename to indicate that this is a user generated file

Copying files
    - Ensure that namespaces are changed and that datasets are the same !!!

Moving Files
    - Moving files in VSCode may cause issues in projects
    - It maybe better to copy the code and create a new file in the necessary folder
    